{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bde14fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f150871",
   "metadata": {},
   "source": [
    "To create tensors that can be updated (weights) we use : Variable , because the other tensors are constant "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dffe8a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(2, 1) dtype=float32, numpy=\n",
      "array([[-0.0320435 ],\n",
      "       [-0.06465375]], dtype=float32)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-15 13:38:09.189789: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\n",
      "2025-09-15 13:38:09.189811: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:178] verbose logging is disabled. Rerun with verbose logging (usually --v=1 or --vmodule=cuda_diagnostics=1) to get more diagnostic output from this module\n",
      "2025-09-15 13:38:09.189815: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:183] retrieving CUDA diagnostic information for host: pepito\n",
      "2025-09-15 13:38:09.189817: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:190] hostname: pepito\n",
      "2025-09-15 13:38:09.189881: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:197] libcuda reported version is: 550.163.1\n",
      "2025-09-15 13:38:09.189893: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:201] kernel reported version is: 550.163.1\n",
      "2025-09-15 13:38:09.189895: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:291] kernel version seems to match DSO: 550.163.1\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(initial_value=tf.random.normal(shape=(2,1), mean=0 , stddev=0.1))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19451213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=3.0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#And now we can modify the variable with the .assign method\n",
    "x[0 , 0].assign(3.0)\n",
    "x[0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47e6a3a",
   "metadata": {},
   "source": [
    "We also have some classic math operation : sqrt,square, id matrice , product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68b16566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1. 1.]\n",
      " [1. 1.]], shape=(2, 2), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[1. 1.]\n",
      " [1. 1.]], shape=(2, 2), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[1. 1.]\n",
      " [1. 1.]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.ones((2,2))\n",
    "b = tf.square(a)\n",
    "c = tf.sqrt(b)\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb3a4e5",
   "metadata": {},
   "source": [
    "Now let's talk about the tape . In a tape , everything is kinda recorded . If we do some operations on a tensor , these operations are kept in the tape and allows tensorflow to calculate the gradient more quickly and efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9aadae6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(6.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Here is an example\n",
    "x = tf.Variable(3.0)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    y = x**2\n",
    "\n",
    "grad = tape.gradient(y,x)\n",
    "print(grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0683ef",
   "metadata": {},
   "source": [
    "We can also use a nested structure where we can compute second-order gradients or more "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f2115f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "tf.Tensor(10.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "time = tf.Variable(0.)\n",
    "with tf.GradientTape() as first_tape:\n",
    "    with tf.GradientTape() as second_tape:\n",
    "        position = 5.0 *time**2\n",
    "    speed = second_tape.gradient(position,time)\n",
    "acceleration = first_tape.gradient(speed, time)\n",
    "\n",
    "print(speed)\n",
    "print(acceleration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8fb52f",
   "metadata": {},
   "source": [
    "Keras "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d93371a",
   "metadata": {},
   "source": [
    "A Layer is an object that encapsulates some state (weights) and some computation\n",
    "(a forward pass). The weights are typically defined in a build() (although they could\n",
    "also be created in the constructor, __init__()), and the computation is defined in\n",
    "the call() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ecef846",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDense(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self,units,activation = None):\n",
    "        super().__init__()\n",
    "        self.units = units\n",
    "        self.activation = activation\n",
    "        \n",
    "    def build(self,input_shape):\n",
    "        input_dim = input_shape[-1]\n",
    "        self.W = self.add_weight(shape=(input_dim,self.units), initialize = \"random_normal\")\n",
    "        self.b = self.add_weight(shape = (self.units,), initialiazer = \"zeros\")\n",
    "    \n",
    "    def call(self,inputs):\n",
    "        y= tf.matmul(inputs, self.W)\n",
    "        if self.activation is not None:\n",
    "            y = self.activation(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbe31d9",
   "metadata": {},
   "source": [
    "Let see how to use keras API to buil a Simple NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c8b13c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras import layers\n",
    "from tensorflow.python.keras import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "248b0eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we use a simple sequantial architecture with two layers\n",
    "model = models.Sequential([\n",
    "    layers.Dense(32,activation=\"relu\"),\n",
    "    layers.Dense(32)\n",
    "])\n",
    "\n",
    "#Use .compile method to configure the trainning process\n",
    "model.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2d29ce",
   "metadata": {},
   "source": [
    "To monitor the loss we use the .fit() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82068f39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
