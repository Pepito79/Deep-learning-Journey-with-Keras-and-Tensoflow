{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b9bdc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from keras.datasets import boston_housing\n",
    "from tensorflow.keras.layers import Dense \n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5341a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/boston_housing.npz\n",
      "\u001b[1m57026/57026\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2us/step\n"
     ]
    }
   ],
   "source": [
    "(train_data , train_result),(test_data,test_results) = boston_housing.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69d577b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404, 13)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae687df4",
   "metadata": {},
   "source": [
    "We need to normalize the data to allow the training to be more easy .\n",
    "We never use the test data , not even for data normalisation . Always use the train data !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd26d8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = train_data.mean(axis=0)\n",
    "train_data -= mean\n",
    "\n",
    "stdd = train_data.std(axis=0)\n",
    "train_data /= stdd\n",
    "\n",
    "test_data -= mean\n",
    "test_data /= stdd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b584224",
   "metadata": {},
   "source": [
    "Now we can build the NN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a14bb5",
   "metadata": {},
   "source": [
    "Why do we use the ReLu activation function ?  \n",
    "\n",
    "For the vanishing gradient issue !\n",
    "\n",
    "In fact , if we use for example the sigmoid function . We know that we use the chain rule do differentiate the Cost function ,  \n",
    "and then σ′(x)=σ(x)(1−σ(x))≤0.25 .  \n",
    "\n",
    "Therefore after a lot of layers the gradient converges to zero and the lowest layers are not learning anything !\n",
    "\n",
    "But if we use the ReLu function : f(x) = max(0,x) . We can assure that the gradient is higher than one  !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a9f6e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = Sequential([\n",
    "        Dense(64,activation=\"relu\"),\n",
    "        Dense(64,activation=\"relu\"),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer = \"rmsprop\", loss='mse' , metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d32738a",
   "metadata": {},
   "source": [
    "To evaluate our model while we keep adjusting its parameters (such as the number of\n",
    "epochs used for training) wre use the K-Fold validation.  \n",
    "\n",
    "It consists of splitting the available data into K partitions (typically K = 4 or 5), instanti-\n",
    "ating K identical models, and training each one on K – 1 partitions while evaluating\n",
    "on the remaining partition. The validation score for the model used is then the aver-\n",
    "age of the K validation scores obtained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80690404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing fold number : 0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Data cardinality is ambiguous. Make sure all arrays contain the same number of samples.'x' sizes: 303\n'y' sizes: 1\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     14\u001b[39m partial_result_train = np.concatenate(\n\u001b[32m     15\u001b[39m     [\n\u001b[32m     16\u001b[39m     test_data[: i*samples_per_fold],\n\u001b[32m     17\u001b[39m     test_data[(i+\u001b[32m1\u001b[39m)*samples_per_fold:]]\n\u001b[32m     18\u001b[39m )\n\u001b[32m     20\u001b[39m model = build_model()\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpartial_data_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpartial_result_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m val_mse, val_mae = model.evaluate(val_data, val_target, verbose=\u001b[32m0\u001b[39m)\n\u001b[32m     23\u001b[39m all_score.append(val_mae)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Python/ML/myenv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Python/ML/myenv/lib/python3.12/site-packages/keras/src/trainers/data_adapters/data_adapter_utils.py:115\u001b[39m, in \u001b[36mcheck_data_cardinality\u001b[39m\u001b[34m(data)\u001b[39m\n\u001b[32m    111\u001b[39m     sizes = \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\n\u001b[32m    112\u001b[39m         \u001b[38;5;28mstr\u001b[39m(i.shape[\u001b[32m0\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tree.flatten(single_data)\n\u001b[32m    113\u001b[39m     )\n\u001b[32m    114\u001b[39m     msg += \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m sizes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msizes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[31mValueError\u001b[39m: Data cardinality is ambiguous. Make sure all arrays contain the same number of samples.'x' sizes: 303\n'y' sizes: 1\n"
     ]
    }
   ],
   "source": [
    "k = 4\n",
    "all_score = []\n",
    "samples_per_fold= len(train_data) // 4\n",
    "for i in range(k):\n",
    "    print(f'Processing fold number : {i}')\n",
    "    val_data = train_data[i * samples_per_fold : (i+1)*samples_per_fold]\n",
    "    val_target = test_data[i * samples_per_fold : (i+1)*samples_per_fold]\n",
    "    \n",
    "    partial_data_train = np.concatenate(\n",
    "        [\n",
    "        train_data[: i*samples_per_fold],\n",
    "        train_data[(i+1)*samples_per_fold:]]\n",
    "    )\n",
    "    partial_result_train = np.concatenate(\n",
    "        [\n",
    "        test_data[: i*samples_per_fold],\n",
    "        test_data[(i+1)*samples_per_fold:]]\n",
    "    )\n",
    "    \n",
    "    model = build_model()\n",
    "    model.fit(partial_data_train,partial_result_train, epochs= 100 , batch_size= 16)\n",
    "    val_mse, val_mae = model.evaluate(val_data, val_target, verbose=0)\n",
    "    all_score.append(val_mae)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
