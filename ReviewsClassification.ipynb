{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ea5705",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-16 13:36:00.325737: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-09-16 13:36:00.335021: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1758022560.345089  127377 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1758022560.348021  127377 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1758022560.356285  127377 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1758022560.356302  127377 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1758022560.356304  127377 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1758022560.356305  127377 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-09-16 13:36:00.359445: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2665934c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.datasets import imdb\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b5dd19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" We will only keep the top 10,000 most fre-\n",
    "quently occurring words in the training data.\"\"\"\n",
    "\n",
    "(train_data, train_labels) , (test_data,test_labels) = imdb.load_data(num_words= 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97dbdf70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_vector(data , dim = 10000):\n",
    "    ''' We define a function that allows us to encode the integer sequences via multi hot encoding'''\n",
    "    res = np.zeros((len(data),dim))\n",
    "    for index , sample in enumerate(data):\n",
    "        for j in sample:\n",
    "            res[index,j] = 1\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112d2cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = to_vector(train_data)\n",
    "x_test = to_vector(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "82d41101",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.asarray(train_labels).astype(\"float32\")\n",
    "y_test = np.asarray(test_labels).astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cb7fc94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(16,activation=\"relu\"),\n",
    "    Dense(16,activation=\"relu\"),\n",
    "    Dense(1,activation=\"sigmoid\"),\n",
    "])\n",
    "\n",
    "model.compile(loss=\"crossentropy\",optimizer=\"rmsprop\", metrics= ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "11bd64bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = x_train[:1000]\n",
    "partial_x_train = x_train[1000:]\n",
    "y_val = y_train[:10000]\n",
    "partial_y_train = y_train[10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "390b0955",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.python.distribute.input_lib' has no attribute 'DistributedDatasetInterface'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m history = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpartial_x_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpartial_y_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Python/ML/myenv/lib/python3.12/site-packages/tensorflow/python/keras/engine/training.py:1137\u001b[39m, in \u001b[36mModel.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[39m\n\u001b[32m   1131\u001b[39m   \u001b[38;5;28mself\u001b[39m._cluster_coordinator = cluster_coordinator.ClusterCoordinator(\n\u001b[32m   1132\u001b[39m       \u001b[38;5;28mself\u001b[39m.distribute_strategy)\n\u001b[32m   1134\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.distribute_strategy.scope(), \\\n\u001b[32m   1135\u001b[39m      training_utils.RespectCompiledTrainableState(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m   1136\u001b[39m   \u001b[38;5;66;03m# Creates a `tf.data.Dataset` and handles batch and epoch iteration.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1137\u001b[39m   data_handler = \u001b[43mdata_adapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_data_handler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1138\u001b[39m \u001b[43m      \u001b[49m\u001b[43mx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1139\u001b[39m \u001b[43m      \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1140\u001b[39m \u001b[43m      \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1141\u001b[39m \u001b[43m      \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1142\u001b[39m \u001b[43m      \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m=\u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1143\u001b[39m \u001b[43m      \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[43m=\u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1144\u001b[39m \u001b[43m      \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1145\u001b[39m \u001b[43m      \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1146\u001b[39m \u001b[43m      \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1147\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1148\u001b[39m \u001b[43m      \u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1149\u001b[39m \u001b[43m      \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1150\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1151\u001b[39m \u001b[43m      \u001b[49m\u001b[43msteps_per_execution\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_steps_per_execution\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1153\u001b[39m   \u001b[38;5;66;03m# Container that configures and calls `tf.keras.Callback`s.\u001b[39;00m\n\u001b[32m   1154\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(callbacks, callbacks_module.CallbackList):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Python/ML/myenv/lib/python3.12/site-packages/tensorflow/python/keras/engine/data_adapter.py:1398\u001b[39m, in \u001b[36mget_data_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1396\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(kwargs[\u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33m_cluster_coordinator\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m   1397\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m _ClusterCoordinatorDataHandler(*args, **kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1398\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataHandler\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Python/ML/myenv/lib/python3.12/site-packages/tensorflow/python/keras/engine/data_adapter.py:1152\u001b[39m, in \u001b[36mDataHandler.__init__\u001b[39m\u001b[34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[39m\n\u001b[32m   1149\u001b[39m   \u001b[38;5;28mself\u001b[39m._steps_per_execution = steps_per_execution\n\u001b[32m   1150\u001b[39m   \u001b[38;5;28mself\u001b[39m._steps_per_execution_value = steps_per_execution.numpy().item()\n\u001b[32m-> \u001b[39m\u001b[32m1152\u001b[39m adapter_cls = \u001b[43mselect_data_adapter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1153\u001b[39m \u001b[38;5;28mself\u001b[39m._adapter = adapter_cls(\n\u001b[32m   1154\u001b[39m     x,\n\u001b[32m   1155\u001b[39m     y,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1164\u001b[39m     distribution_strategy=distribute_lib.get_strategy(),\n\u001b[32m   1165\u001b[39m     model=model)\n\u001b[32m   1167\u001b[39m strategy = distribute_lib.get_strategy()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Python/ML/myenv/lib/python3.12/site-packages/tensorflow/python/keras/engine/data_adapter.py:988\u001b[39m, in \u001b[36mselect_data_adapter\u001b[39m\u001b[34m(x, y)\u001b[39m\n\u001b[32m    986\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mselect_data_adapter\u001b[39m(x, y):\n\u001b[32m    987\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Selects a data adapter than can handle a given x and y.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m988\u001b[39m   adapter_cls = [\u001b[38;5;28mcls\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01min\u001b[39;00m ALL_ADAPTER_CLS \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcan_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[32m    989\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m adapter_cls:\n\u001b[32m    990\u001b[39m     \u001b[38;5;66;03m# TODO(scottzhu): This should be a less implementation-specific error.\u001b[39;00m\n\u001b[32m    991\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    992\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFailed to find data adapter that can handle \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    993\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33minput: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\n\u001b[32m    994\u001b[39m             _type_name(x), _type_name(y)))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Python/ML/myenv/lib/python3.12/site-packages/tensorflow/python/keras/engine/data_adapter.py:707\u001b[39m, in \u001b[36mDatasetAdapter.can_handle\u001b[39m\u001b[34m(x, y)\u001b[39m\n\u001b[32m    704\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcan_handle\u001b[39m(x, y=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    706\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(x, (data_types.DatasetV1, data_types.DatasetV2)) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m707\u001b[39m           \u001b[43m_is_distributed_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Python/ML/myenv/lib/python3.12/site-packages/tensorflow/python/keras/engine/data_adapter.py:1697\u001b[39m, in \u001b[36m_is_distributed_dataset\u001b[39m\u001b[34m(ds)\u001b[39m\n\u001b[32m   1696\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_is_distributed_dataset\u001b[39m(ds):\n\u001b[32m-> \u001b[39m\u001b[32m1697\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ds, \u001b[43minput_lib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDistributedDatasetInterface\u001b[49m)\n",
      "\u001b[31mAttributeError\u001b[39m: module 'tensorflow.python.distribute.input_lib' has no attribute 'DistributedDatasetInterface'"
     ]
    }
   ],
   "source": [
    "history = model.fit(partial_x_train,partial_y_train,epochs=20,batch_size=512,validation_data=(x_val, y_val))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
